{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf88b46d",
   "metadata": {},
   "source": [
    "# Data Workflow Project (P1)\n",
    "**Name:** Christopher Aaron O'Hara\n",
    "\n",
    "**Dataset:** OpenRCA (AIOps telemetry incidents)  \n",
    "\n",
    "**Dataset Link:** https://github.com/microsoft/OpenRCA\n",
    "\n",
    "This notebook implements a complete, reproducible AIOps data workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acab6c7",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "192fd5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1enBrdPT3wLG94ITGbSOwUFg9fkLR-16R Bank.zip\n",
      "Processing file 1aDbqFHhYGgywLDfq54O7Ln8wgiv-BqGJ Market.zip\n",
      "Processing file 1cyOKpqyAP4fy-QiJ6a_cKuwR7D46zyVe Telecom.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1enBrdPT3wLG94ITGbSOwUFg9fkLR-16R\n",
      "From (redirected): https://drive.google.com/uc?id=1enBrdPT3wLG94ITGbSOwUFg9fkLR-16R&confirm=t&uuid=cf8f5af6-9a26-4e86-8005-24ebb7588d63\n",
      "To: c:\\Users\\Ohara\\Desktop\\__Udacity\\__MSc_Project\\P1\\data\\openrca\\Bank.zip\n",
      " 41%|████      | 1.15G/2.83G [05:51<10:46, 2.60MB/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[119]\u001b[39m\u001b[32m, line 195\u001b[39m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m subset, metadata\n\u001b[32m    190\u001b[39m aif360_ready, aif360_status = ensure_python_package(\n\u001b[32m    191\u001b[39m     module_name=\u001b[33m\"\u001b[39m\u001b[33maif360\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    192\u001b[39m     package_spec=AIF360_PACKAGE_SPEC,\n\u001b[32m    193\u001b[39m     auto_install=AUTO_INSTALL_AIF360,\n\u001b[32m    194\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m openrca_zip_ready, openrca_zip_status, openrca_query_path = \u001b[43mmaybe_download_and_extract_openrca_zip\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgdrive_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOPENRCA_TELECOM_ZIP_GDRIVE_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDOWNLOADED_OPENRCA_ZIP_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOPENRCA_EXTRACT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauto_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAUTO_DOWNLOAD_OPENRCA_TELECOM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem_preference\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOPENRCA_SYSTEM_PREFERENCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m dataset_ready, dataset_status = maybe_download_dataset(\n\u001b[32m    203\u001b[39m     url=DATASET_URL,\n\u001b[32m    204\u001b[39m     output_path=DOWNLOADED_DATASET_PATH,\n\u001b[32m    205\u001b[39m     auto_download=AUTO_DOWNLOAD_DATASET,\n\u001b[32m    206\u001b[39m )\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m openrca_query_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[119]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mmaybe_download_and_extract_openrca_zip\u001b[39m\u001b[34m(gdrive_url, zip_path, extract_dir, auto_download, system_preference)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m/folders/\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m gdrive_url:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m         \u001b[43mgdown\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgdrive_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextract_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m            \u001b[49m\u001b[43mremaining_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m         query_after_folder = find_openrca_query_csv(extract_dir, system_preference=system_preference)\n\u001b[32m    120\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m query_after_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gdown\\download_folder.py:325\u001b[39m, in \u001b[36mdownload_folder\u001b[39m\u001b[34m(url, id, output, quiet, proxy, speed, use_cookies, remaining_ok, verify, user_agent, skip_download, resume)\u001b[39m\n\u001b[32m    322\u001b[39m     files.append(local_path)\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m local_path = \u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://drive.google.com/uc?id=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cookies\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gdown\\download.py:369\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[39m\n\u001b[32m    367\u001b[39m t_start = time.time()\n\u001b[32m    368\u001b[39m downloaded = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownloaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py:1060\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1059\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1062\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1063\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py:949\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    946\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    947\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py:873\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    870\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    874\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    875\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    876\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    882\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    883\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\response.py:856\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    853\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    854\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    855\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\http\\client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 1.15G/2.83G [06:10<10:46, 2.60MB/s]"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "import urllib.request\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Bootstrap toggles\n",
    "AUTO_INSTALL_AIF360 = True\n",
    "AUTO_DOWNLOAD_DATASET = False\n",
    "AUTO_DOWNLOAD_OPENRCA_TELECOM = True\n",
    "AIF360_PACKAGE_SPEC = \"aif360==0.6.1\"\n",
    "GDOWN_PACKAGE_SPEC = \"gdown>=5.2.0\"\n",
    "DATASET_URL = \"\"  # Optional direct CSV URL. OpenRCA full dataset is typically downloaded manually from Google Drive.\n",
    "OPENRCA_TELECOM_ZIP_GDRIVE_URL = \"https://drive.google.com/drive/folders/1wGiEnu4OkWrjPxfx5ZTROnU37-5UDoPM?usp=drive_link\"  # Folder or direct file URL.\n",
    "OPENRCA_TELECOM_ZIP_FILE_ID = \"1cyOKpqyAP4fy-QiJ6a_cKuwR7D46zyVe\"  # Telecom.zip file id to avoid downloading Bank/Market.\n",
    "OPENRCA_DRIVE_URL = \"https://drive.google.com/drive/folders/1wGiEnu4OkWrjPxfx5ZTROnU37-5UDoPM?usp=drive_link\"\n",
    "OPENRCA_SYSTEM_PREFERENCE = \"Telecom\"\n",
    "OPENRCA_EXTRACT_DIR = Path(\"data\") / \"openrca\"\n",
    "DOWNLOADED_OPENRCA_ZIP_PATH = Path(\"data\") / \"openrca_telecom.zip\"\n",
    "DOWNLOADED_DATASET_PATH = OPENRCA_EXTRACT_DIR / \"Telecom\" / \"query.csv\"\n",
    "\n",
    "# Subset controls for faster exploratory workflow\n",
    "SUBSET_ENABLED = True\n",
    "SUBSET_MAX_ROWS = 50000\n",
    "SUBSET_RANDOM_STATE = 42\n",
    "SUBSET_STRATIFY_COLUMN = None  # Example: \"service\" or \"fault_type\" if present\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "\n",
    "\n",
    "def ensure_python_package(module_name: str, package_spec: str, auto_install: bool = False) -> tuple[bool, str]:\n",
    "    \"\"\"Ensure a Python package is importable; optionally install it in the active kernel environment.\"\"\"\n",
    "    if importlib.util.find_spec(module_name) is not None:\n",
    "        return True, \"already_installed\"\n",
    "    if not auto_install:\n",
    "        return False, \"missing_auto_install_disabled\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_spec])\n",
    "        ok = importlib.util.find_spec(module_name) is not None\n",
    "        return ok, \"installed\" if ok else \"install_failed_import\"\n",
    "    except Exception as exc:\n",
    "        return False, f\"install_error: {exc}\"\n",
    "\n",
    "\n",
    "def maybe_download_dataset(url: str, output_path: Path, auto_download: bool = False) -> tuple[bool, str]:\n",
    "    \"\"\"Download a dataset file only when enabled and no local copy exists.\"\"\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if output_path.exists():\n",
    "        return True, \"already_present\"\n",
    "    if not auto_download:\n",
    "        return False, \"download_disabled\"\n",
    "    if not url:\n",
    "        return False, \"missing_dataset_url\"\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, output_path)\n",
    "        return output_path.exists(), \"downloaded\"\n",
    "    except Exception as exc:\n",
    "        return False, f\"download_error: {exc}\"\n",
    "\n",
    "\n",
    "def find_openrca_query_csv(base_dir: Path, system_preference: str = \"Telecom\") -> Path | None:\n",
    "    \"\"\"Find OpenRCA query.csv, preferring paths that contain the requested system name.\"\"\"\n",
    "    query_files = sorted(base_dir.glob(\"**/query.csv\"), key=lambda x: str(x).lower())\n",
    "    if not query_files:\n",
    "        return None\n",
    "    preferred = [p for p in query_files if system_preference.lower() in str(p).lower()]\n",
    "    return preferred[0] if preferred else query_files[0]\n",
    "\n",
    "\n",
    "def maybe_download_and_extract_openrca_zip(\n",
    "    gdrive_url: str,\n",
    "    telecom_zip_file_id: str,\n",
    "    zip_path: Path,\n",
    "    extract_dir: Path,\n",
    "    auto_download: bool = False,\n",
    "    system_preference: str = \"Telecom\",\n",
    ") -> tuple[bool, str, Path | None]:\n",
    "    \"\"\"Download OpenRCA ZIP from Google Drive and extract query.csv files for ingestion.\"\"\"\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "    existing_query = find_openrca_query_csv(extract_dir, system_preference=system_preference)\n",
    "    if existing_query is not None:\n",
    "        return True, \"already_present\", existing_query\n",
    "    if not auto_download:\n",
    "        return False, \"download_disabled\", None\n",
    "    if not gdrive_url and not telecom_zip_file_id:\n",
    "        return False, \"missing_gdrive_url_or_file_id\", None\n",
    "\n",
    "    gdown_ready, gdown_status = ensure_python_package(\n",
    "        module_name=\"gdown\",\n",
    "        package_spec=GDOWN_PACKAGE_SPEC,\n",
    "        auto_install=True,\n",
    "    )\n",
    "    if not gdown_ready:\n",
    "        return False, f\"gdown_unavailable:{gdown_status}\", None\n",
    "\n",
    "    try:\n",
    "        import gdown\n",
    "    except Exception as exc:\n",
    "        return False, f\"gdown_import_error:{exc}\", None\n",
    "\n",
    "    try:\n",
    "        zip_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if telecom_zip_file_id:\n",
    "            downloaded_file = gdown.download(\n",
    "                id=telecom_zip_file_id,\n",
    "                output=str(zip_path),\n",
    "                quiet=False,\n",
    "                fuzzy=True,\n",
    "            )\n",
    "            if downloaded_file is None or not zip_path.exists():\n",
    "                return False, \"telecom_zip_download_failed\", None\n",
    "        elif gdrive_url and \"/folders/\" in gdrive_url:\n",
    "            return False, \"folder_url_requires_telecom_file_id\", None\n",
    "        else:\n",
    "            downloaded_file = gdown.download(\n",
    "                url=gdrive_url,\n",
    "                output=str(zip_path),\n",
    "                quiet=False,\n",
    "                fuzzy=True,\n",
    "            )\n",
    "            if downloaded_file is None or not zip_path.exists():\n",
    "                return False, \"zip_download_failed\", None\n",
    "    except Exception as exc:\n",
    "        return False, f\"zip_download_error:{exc}\", None\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "            zf.extractall(extract_dir)\n",
    "    except Exception as exc:\n",
    "        return False, f\"zip_extract_error:{exc}\", None\n",
    "\n",
    "    extracted_query = find_openrca_query_csv(extract_dir, system_preference=system_preference)\n",
    "    if extracted_query is None:\n",
    "        return False, \"extracted_but_query_missing\", None\n",
    "    return True, \"downloaded_and_extracted\", extracted_query\n",
    "\n",
    "\n",
    "def maybe_subset_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    enabled: bool = True,\n",
    "    max_rows: int | None = 50000,\n",
    "    random_state: int = 42,\n",
    "    stratify_col: str | None = None,\n",
    ") -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"Return a reproducible subset for EDA while tracking subset metadata for V&V.\"\"\"\n",
    "    metadata = {\n",
    "        \"subset_enabled\": enabled,\n",
    "        \"subset_applied\": False,\n",
    "        \"original_rows\": int(len(df)),\n",
    "        \"subset_rows\": int(len(df)),\n",
    "        \"stratified\": False,\n",
    "        \"stratify_col\": stratify_col,\n",
    "    }\n",
    "\n",
    "    if not enabled or max_rows is None or len(df) <= max_rows:\n",
    "        return df, metadata\n",
    "\n",
    "    if stratify_col and stratify_col in df.columns and df[stratify_col].nunique(dropna=True) > 1:\n",
    "        sampled = (\n",
    "            df.groupby(stratify_col, group_keys=False)\n",
    "            .apply(lambda part: part.sample(n=max(1, int(round(len(part) * max_rows / len(df)))), random_state=random_state))\n",
    "        )\n",
    "        if len(sampled) > max_rows:\n",
    "            sampled = sampled.sample(n=max_rows, random_state=random_state)\n",
    "        subset = sampled.reset_index(drop=True)\n",
    "        metadata[\"stratified\"] = True\n",
    "    else:\n",
    "        subset = df.sample(n=max_rows, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    metadata[\"subset_applied\"] = True\n",
    "    metadata[\"subset_rows\"] = int(len(subset))\n",
    "    return subset, metadata\n",
    "\n",
    "\n",
    "aif360_ready, aif360_status = ensure_python_package(\n",
    "    module_name=\"aif360\",\n",
    "    package_spec=AIF360_PACKAGE_SPEC,\n",
    "    auto_install=AUTO_INSTALL_AIF360,\n",
    ")\n",
    "openrca_zip_ready, openrca_zip_status, openrca_query_path = maybe_download_and_extract_openrca_zip(\n",
    "    gdrive_url=OPENRCA_TELECOM_ZIP_GDRIVE_URL,\n",
    "    telecom_zip_file_id=OPENRCA_TELECOM_ZIP_FILE_ID,\n",
    "    zip_path=DOWNLOADED_OPENRCA_ZIP_PATH,\n",
    "    extract_dir=OPENRCA_EXTRACT_DIR,\n",
    "    auto_download=AUTO_DOWNLOAD_OPENRCA_TELECOM,\n",
    "    system_preference=OPENRCA_SYSTEM_PREFERENCE,\n",
    ")\n",
    "dataset_ready, dataset_status = maybe_download_dataset(\n",
    "    url=DATASET_URL,\n",
    "    output_path=DOWNLOADED_DATASET_PATH,\n",
    "    auto_download=AUTO_DOWNLOAD_DATASET,\n",
    ")\n",
    "if openrca_query_path is not None:\n",
    "    DOWNLOADED_DATASET_PATH = openrca_query_path\n",
    "\n",
    "try:\n",
    "    from aif360.datasets import BinaryLabelDataset\n",
    "    from aif360.metrics import BinaryLabelDatasetMetric\n",
    "    AIF360_AVAILABLE = True\n",
    "except Exception:\n",
    "    AIF360_AVAILABLE = False\n",
    "\n",
    "BOOTSTRAP_STATUS = {\n",
    "    \"aif360_status\": aif360_status,\n",
    "    \"openrca_zip_status\": openrca_zip_status,\n",
    "    \"openrca_zip_ready\": openrca_zip_ready,\n",
    "    \"openrca_query_path\": str(openrca_query_path) if openrca_query_path is not None else None,\n",
    "    \"dataset_status\": dataset_status,\n",
    "    \"download_target\": str(DOWNLOADED_DATASET_PATH),\n",
    "    \"openrca_drive_url\": OPENRCA_DRIVE_URL,\n",
    "    \"openrca_zip_url\": OPENRCA_TELECOM_ZIP_GDRIVE_URL,\n",
    "    \"openrca_zip_file_id\": OPENRCA_TELECOM_ZIP_FILE_ID,\n",
    "    \"openrca_system_preference\": OPENRCA_SYSTEM_PREFERENCE,\n",
    "}\n",
    "print(\"Bootstrap status:\", BOOTSTRAP_STATUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "07e11eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V&V Setup: {'core_imports_loaded': True, 'aif360_available': True, 'matplotlib_backend_ready': True, 'bootstrap_status_present': True, 'subset_controls_present': True, 'telecom_bootstrap_controls_present': True}\n",
      "Bootstrap status: {'aif360_status': 'already_installed', 'openrca_zip_status': 'download_disabled', 'openrca_zip_ready': False, 'openrca_query_path': None, 'dataset_status': 'download_disabled', 'download_target': 'data\\\\openrca\\\\Telecom\\\\query.csv', 'openrca_drive_url': 'https://drive.google.com/drive/folders/1wGiEnu4OkWrjPxfx5ZTROnU37-5UDoPM?usp=drive_link', 'openrca_zip_url': '', 'openrca_system_preference': 'Telecom'}\n",
      "OpenRCA file missing. Set AUTO_DOWNLOAD_OPENRCA_TELECOM=True and provide OPENRCA_TELECOM_ZIP_GDRIVE_URL, then rerun Setup.\n",
      "Alternative: place CSVs under data/openrca/{SYSTEM}/query.csv manually.\n",
      "Official OpenRCA data folder: https://drive.google.com/drive/folders/1wGiEnu4OkWrjPxfx5ZTROnU37-5UDoPM?usp=drive_link\n"
     ]
    }
   ],
   "source": [
    "setup_checks = {\n",
    "    \"core_imports_loaded\": all(name in globals() for name in [\"np\", \"pd\", \"plt\", \"sns\", \"TSNE\", \"StandardScaler\"]),\n",
    "    \"aif360_available\": AIF360_AVAILABLE,\n",
    "    \"matplotlib_backend_ready\": \"plt\" in globals(),\n",
    "    \"bootstrap_status_present\": \"BOOTSTRAP_STATUS\" in globals(),\n",
    "    \"subset_controls_present\": all(name in globals() for name in [\"SUBSET_ENABLED\", \"SUBSET_MAX_ROWS\", \"SUBSET_RANDOM_STATE\"]),\n",
    "    \"telecom_bootstrap_controls_present\": all(name in globals() for name in [\"AUTO_DOWNLOAD_OPENRCA_TELECOM\", \"OPENRCA_TELECOM_ZIP_GDRIVE_URL\", \"OPENRCA_TELECOM_ZIP_FILE_ID\", \"OPENRCA_SYSTEM_PREFERENCE\"]),\n",
    "}\n",
    "print(\"V&V Setup:\", setup_checks)\n",
    "print(\"Bootstrap status:\", BOOTSTRAP_STATUS)\n",
    "if not AIF360_AVAILABLE:\n",
    "    print(\"AIF360 is not available. Set AUTO_INSTALL_AIF360=True (or install manually) and rerun Setup.\")\n",
    "if not DOWNLOADED_DATASET_PATH.exists():\n",
    "    print(\"OpenRCA file missing. Set AUTO_DOWNLOAD_OPENRCA_TELECOM=True and verify OPENRCA_TELECOM_ZIP_FILE_ID, then rerun Setup.\")\n",
    "    print(\"Alternative: place CSVs under data/openrca/{SYSTEM}/query.csv manually.\")\n",
    "    print(f\"Official OpenRCA data folder: {OPENRCA_DRIVE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd577f2",
   "metadata": {},
   "source": [
    "## 2) Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "25d94b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No OpenRCA CSV found. Put files under data/openrca/{SYSTEM}/query.csv and rerun.\n",
      "Expected systems: Bank, Telecom, Market/cloudbed-1, Market/cloudbed-2\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "bootstrap_dataset_path = Path(globals().get(\"DOWNLOADED_DATASET_PATH\", DATA_DIR / \"openrca\" / \"Telecom\" / \"query.csv\"))\n",
    "openrca_preferred_paths = [\n",
    "    bootstrap_dataset_path,\n",
    "    DATA_DIR / \"openrca\" / \"Telecom\" / \"query.csv\",\n",
    "    DATA_DIR / \"openrca\" / \"Bank\" / \"query.csv\",\n",
    "    DATA_DIR / \"openrca\" / \"Market\" / \"cloudbed-1\" / \"query.csv\",\n",
    "    DATA_DIR / \"openrca\" / \"Market\" / \"cloudbed-2\" / \"query.csv\",\n",
    "]\n",
    "PREFERRED_DATASET_PATH = next((p for p in openrca_preferred_paths if p.exists()), None)\n",
    "\n",
    "openrca_query_csvs = sorted(DATA_DIR.glob(\"**/query.csv\"), key=lambda x: str(x).lower())\n",
    "system_preference = str(globals().get(\"OPENRCA_SYSTEM_PREFERENCE\", \"Telecom\")).lower()\n",
    "preferred_query_csvs = [p for p in openrca_query_csvs if system_preference in str(p).lower()]\n",
    "fallback_query_csvs = [p for p in openrca_query_csvs if p not in preferred_query_csvs]\n",
    "generic_csvs = sorted(\n",
    "    [p for p in DATA_DIR.glob(\"*.csv\")] + [p for p in Path(\".\").glob(\"*.csv\")],\n",
    "    key=lambda x: str(x).lower(),\n",
    ")\n",
    "candidate_csvs = preferred_query_csvs + fallback_query_csvs + generic_csvs\n",
    "\n",
    "if PREFERRED_DATASET_PATH is not None:\n",
    "    DATASET_PATH = PREFERRED_DATASET_PATH\n",
    "elif candidate_csvs:\n",
    "    DATASET_PATH = candidate_csvs[0]\n",
    "else:\n",
    "    DATASET_PATH = None\n",
    "\n",
    "if DATASET_PATH is None:\n",
    "    df_raw = None\n",
    "    SUBSET_METADATA = None\n",
    "    print(\"No OpenRCA CSV found. Put files under data/openrca/{SYSTEM}/query.csv and rerun.\")\n",
    "    print(\"Expected systems: Bank, Telecom, Market/cloudbed-1, Market/cloudbed-2\")\n",
    "else:\n",
    "    df_raw = pd.read_csv(DATASET_PATH, low_memory=False)\n",
    "    df_raw, SUBSET_METADATA = maybe_subset_dataframe(\n",
    "        df_raw,\n",
    "        enabled=SUBSET_ENABLED,\n",
    "        max_rows=SUBSET_MAX_ROWS,\n",
    "        random_state=SUBSET_RANDOM_STATE,\n",
    "        stratify_col=SUBSET_STRATIFY_COLUMN,\n",
    "    )\n",
    "    print(f\"Loaded dataset: {DATASET_PATH}\")\n",
    "    print(f\"Subset metadata: {SUBSET_METADATA}\")\n",
    "    print(f\"Shape: {df_raw.shape[0]} rows x {df_raw.shape[1]} columns\")\n",
    "    display(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fc66ec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V&V Ingestion: pending (dataset missing)\n"
     ]
    }
   ],
   "source": [
    "if df_raw is None:\n",
    "    print(\"V&V Ingestion: pending (dataset missing)\")\n",
    "else:\n",
    "    time_like_cols = [c for c in df_raw.columns if any(k in c.lower() for k in [\"time\", \"date\", \"timestamp\"])]\n",
    "    ingestion_checks = {\n",
    "        \"is_dataframe\": isinstance(df_raw, pd.DataFrame),\n",
    "        \"row_count_at_least_200\": df_raw.shape[0] >= 200,\n",
    "        \"column_count_at_least_5\": df_raw.shape[1] >= 5,\n",
    "        \"has_time_like_column\": len(time_like_cols) > 0,\n",
    "        \"subset_metadata_present\": isinstance(globals().get(\"SUBSET_METADATA\"), dict),\n",
    "        \"subset_applied\": bool(globals().get(\"SUBSET_METADATA\", {}).get(\"subset_applied\", False)),\n",
    "    }\n",
    "    print(\"V&V Ingestion:\", ingestion_checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c7888c",
   "metadata": {},
   "source": [
    "## 3) Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b1f4d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert columns to lowercase snake_case for reliable downstream code.\"\"\"\n",
    "    cleaned = df.copy()\n",
    "    cleaned.columns = (\n",
    "        cleaned.columns.str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.replace(\"-\", \"_\", regex=False)\n",
    "    )\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def drop_duplicate_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove exact duplicate records to avoid duplicated incident evidence.\"\"\"\n",
    "    return df.drop_duplicates().copy()\n",
    "\n",
    "\n",
    "def parse_time_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Parse columns containing time/date/timestamp into datetime where possible.\"\"\"\n",
    "    cleaned = df.copy()\n",
    "    time_cols = [c for c in cleaned.columns if any(k in c for k in [\"time\", \"date\", \"timestamp\"])]\n",
    "    for col in time_cols:\n",
    "        cleaned[col] = pd.to_datetime(cleaned[col], errors=\"coerce\")\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def impute_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Impute numeric columns by median and categorical columns by mode for robust EDA continuity.\"\"\"\n",
    "    cleaned = df.copy()\n",
    "\n",
    "    num_cols = cleaned.select_dtypes(include=[np.number]).columns\n",
    "    cat_cols = cleaned.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns\n",
    "\n",
    "    if len(num_cols) > 0:\n",
    "        cleaned[num_cols] = SimpleImputer(strategy=\"median\").fit_transform(cleaned[num_cols])\n",
    "\n",
    "    for col in cat_cols:\n",
    "        if cleaned[col].isna().any():\n",
    "            mode_vals = cleaned[col].mode(dropna=True)\n",
    "            if len(mode_vals) > 0:\n",
    "                cleaned[col] = cleaned[col].fillna(mode_vals.iloc[0])\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "971694cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning skipped until dataset is available.\n"
     ]
    }
   ],
   "source": [
    "if df_raw is None:\n",
    "    df_clean = None\n",
    "    print(\"Cleaning skipped until dataset is available.\")\n",
    "else:\n",
    "    df_clean = standardize_column_names(df_raw)\n",
    "    df_clean = drop_duplicate_rows(df_clean)\n",
    "    df_clean = parse_time_columns(df_clean)\n",
    "    df_clean = impute_missing_values(df_clean)\n",
    "    print(\"Cleaning complete.\")\n",
    "    print(f\"Cleaned shape: {df_clean.shape[0]} rows x {df_clean.shape[1]} columns\")\n",
    "    display(df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "05580a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V&V Cleaning: pending (dataset missing)\n"
     ]
    }
   ],
   "source": [
    "if df_clean is None:\n",
    "    print(\"V&V Cleaning: pending (dataset missing)\")\n",
    "else:\n",
    "    numeric_missing = int(df_clean.select_dtypes(include=[np.number]).isna().sum().sum())\n",
    "    cleaning_checks = {\n",
    "        \"functions_defined\": all(\n",
    "            fn in globals()\n",
    "            for fn in [\"standardize_column_names\", \"drop_duplicate_rows\", \"parse_time_columns\", \"impute_missing_values\"]\n",
    "        ),\n",
    "        \"duplicate_rows_remaining\": int(df_clean.duplicated().sum()),\n",
    "        \"numeric_missing_after_cleaning\": numeric_missing,\n",
    "    }\n",
    "    print(\"V&V Cleaning:\", cleaning_checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c024f0a7",
   "metadata": {},
   "source": [
    "## 4) Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "03fa639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eda_report(df: pd.DataFrame) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"Build core EDA artifacts: summary stats, missingness profile, cardinality, and numeric correlations.\"\"\"\n",
    "    summary_stats = df.describe(include=\"all\", datetime_is_numeric=True).transpose()\n",
    "    missingness = (\n",
    "        df.isna().sum()\n",
    "        .rename(\"missing_count\")\n",
    "        .to_frame()\n",
    "        .assign(missing_pct=lambda x: (x[\"missing_count\"] / len(df)) * 100)\n",
    "        .sort_values(\"missing_count\", ascending=False)\n",
    "    )\n",
    "    cardinality = (\n",
    "        df.nunique(dropna=True)\n",
    "        .rename(\"unique_values\")\n",
    "        .to_frame()\n",
    "        .sort_values(\"unique_values\", ascending=False)\n",
    "    )\n",
    "    numeric_corr = df.select_dtypes(include=[np.number]).corr(numeric_only=True)\n",
    "    return {\n",
    "        \"summary_stats\": summary_stats,\n",
    "        \"missingness\": missingness,\n",
    "        \"cardinality\": cardinality,\n",
    "        \"numeric_corr\": numeric_corr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cfe3158b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA skipped until dataset is available.\n"
     ]
    }
   ],
   "source": [
    "if df_clean is None:\n",
    "    eda_report = None\n",
    "    print(\"EDA skipped until dataset is available.\")\n",
    "else:\n",
    "    eda_report = generate_eda_report(df_clean)\n",
    "    print(\"Summary statistics (top 12 rows):\")\n",
    "    display(eda_report[\"summary_stats\"].head(12))\n",
    "    print(\"Missingness (top 12 rows):\")\n",
    "    display(eda_report[\"missingness\"].head(12))\n",
    "    print(\"Cardinality (top 12 rows):\")\n",
    "    display(eda_report[\"cardinality\"].head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b8b5ad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V&V EDA: pending (dataset missing)\n"
     ]
    }
   ],
   "source": [
    "if eda_report is None:\n",
    "    print(\"V&V EDA: pending (dataset missing)\")\n",
    "else:\n",
    "    eda_checks = {\n",
    "        \"eda_function_defined\": \"generate_eda_report\" in globals(),\n",
    "        \"summary_stats_present\": \"summary_stats\" in eda_report,\n",
    "        \"missingness_present\": \"missingness\" in eda_report,\n",
    "        \"cardinality_present\": \"cardinality\" in eda_report,\n",
    "        \"correlation_present\": \"numeric_corr\" in eda_report,\n",
    "    }\n",
    "    print(\"V&V EDA:\", eda_checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f2c9d0",
   "metadata": {},
   "source": [
    "## 5) Visualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "56211e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization 1 skipped: missing EDA report.\n"
     ]
    }
   ],
   "source": [
    "if eda_report is not None:\n",
    "    top_missing = eda_report[\"missingness\"].head(15).reset_index().rename(columns={\"index\": \"column\"})\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(data=top_missing, x=\"column\", y=\"missing_pct\")\n",
    "    plt.title(\"Top Columns by Missing Percentage\")\n",
    "    plt.xlabel(\"Column\")\n",
    "    plt.ylabel(\"Missing Values (%)\")\n",
    "    plt.xticks(rotation=60, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Visualization 1 skipped: missing EDA report.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f079b0",
   "metadata": {},
   "source": [
    "**Figure 1 interpretation:** This plot identifies data-quality risk areas and guides cleaning priorities for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ce0f4cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization 2 skipped: not enough numeric features for correlation heatmap.\n"
     ]
    }
   ],
   "source": [
    "if eda_report is not None and not eda_report[\"numeric_corr\"].empty:\n",
    "    corr = eda_report[\"numeric_corr\"]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "    plt.title(\"Numeric Feature Correlation Heatmap\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Visualization 2 skipped: not enough numeric features for correlation heatmap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2c153",
   "metadata": {},
   "source": [
    "**Figure 2 interpretation:** Correlation structure highlights redundant metrics and potential service interactions to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4689cc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization 3 skipped: dataset missing.\n"
     ]
    }
   ],
   "source": [
    "if df_clean is None:\n",
    "    tsne_frame = None\n",
    "    print(\"Visualization 3 skipped: dataset missing.\")\n",
    "else:\n",
    "    numeric_df = df_clean.select_dtypes(include=[np.number]).copy()\n",
    "    numeric_df = numeric_df.loc[:, numeric_df.nunique(dropna=True) > 1]\n",
    "\n",
    "    if numeric_df.shape[0] < 10 or numeric_df.shape[1] < 2:\n",
    "        tsne_frame = None\n",
    "        print(\"Visualization 3 skipped: need at least 10 rows and 2 informative numeric features.\")\n",
    "    else:\n",
    "        sample_n = min(2000, len(numeric_df))\n",
    "        sampled_numeric = numeric_df.sample(n=sample_n, random_state=42)\n",
    "        X = StandardScaler().fit_transform(sampled_numeric)\n",
    "\n",
    "        perplexity = min(30, max(5, sample_n // 20))\n",
    "        if perplexity >= sample_n:\n",
    "            perplexity = max(2, sample_n - 1)\n",
    "\n",
    "        embedding = TSNE(\n",
    "            n_components=2,\n",
    "            init=\"random\",\n",
    "            learning_rate=\"auto\",\n",
    "            perplexity=perplexity,\n",
    "            random_state=42,\n",
    "        ).fit_transform(X)\n",
    "\n",
    "        tsne_frame = pd.DataFrame({\"tsne_1\": embedding[:, 0], \"tsne_2\": embedding[:, 1]}, index=sampled_numeric.index)\n",
    "\n",
    "        hue_col = None\n",
    "        label_candidates = [\n",
    "            \"label\", \"is_anomaly\", \"anomaly\", \"incident\", \"fault_type\", \"root_cause_service\", \"service\"\n",
    "        ]\n",
    "        for col in label_candidates:\n",
    "            if col in df_clean.columns and df_clean[col].nunique(dropna=True) <= 10:\n",
    "                hue_col = col\n",
    "                break\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        if hue_col is None:\n",
    "            sns.scatterplot(data=tsne_frame, x=\"tsne_1\", y=\"tsne_2\", s=25)\n",
    "        else:\n",
    "            tsne_frame[hue_col] = df_clean.loc[tsne_frame.index, hue_col].astype(str)\n",
    "            sns.scatterplot(data=tsne_frame, x=\"tsne_1\", y=\"tsne_2\", hue=hue_col, s=25)\n",
    "        plt.title(\"t-SNE Projection of Numeric AIOps Features\")\n",
    "        plt.xlabel(\"t-SNE Component 1\")\n",
    "        plt.ylabel(\"t-SNE Component 2\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a1f8f",
   "metadata": {},
   "source": [
    "**Figure 3 interpretation:** t-SNE reveals local structure and possible cluster separation not visible in univariate summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ad533171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V&V Visualizations: pending (dataset missing)\n"
     ]
    }
   ],
   "source": [
    "if df_clean is None:\n",
    "    print(\"V&V Visualizations: pending (dataset missing)\")\n",
    "else:\n",
    "    viz_checks = {\n",
    "        \"viz1_missingness_ready\": eda_report is not None,\n",
    "        \"viz2_correlation_ready\": eda_report is not None and not eda_report[\"numeric_corr\"].empty,\n",
    "        \"viz3_tsne_attempted\": tsne_frame is None or isinstance(tsne_frame, pd.DataFrame),\n",
    "    }\n",
    "    print(\"V&V Visualizations:\", viz_checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1905a8",
   "metadata": {},
   "source": [
    "## 6) EDA Bias/Fairness Check with AIF360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8dc4fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_fairness_columns(df: pd.DataFrame) -> tuple[str | None, str | None]:\n",
    "    \"\"\"Infer one label column and one protected-group proxy column for an AIF360 screening audit.\"\"\"\n",
    "    label_priority = [\"is_anomaly\", \"anomaly\", \"incident\", \"label\", \"target\", \"fault_type\"]\n",
    "    group_priority = [\"service\", \"service_name\", \"team\", \"system\", \"cluster\", \"namespace\", \"region\", \"environment\", \"env\", \"host\"]\n",
    "\n",
    "    label_col = None\n",
    "    for col in label_priority:\n",
    "        if col in df.columns and df[col].nunique(dropna=True) >= 2:\n",
    "            label_col = col\n",
    "            break\n",
    "    if label_col is None:\n",
    "        for col in df.columns:\n",
    "            if df[col].nunique(dropna=True) == 2:\n",
    "                label_col = col\n",
    "                break\n",
    "\n",
    "    group_col = None\n",
    "    for col in group_priority:\n",
    "        if col in df.columns and col != label_col and df[col].nunique(dropna=True) >= 2:\n",
    "            group_col = col\n",
    "            break\n",
    "    if group_col is None:\n",
    "        for col in df.columns:\n",
    "            if col != label_col and df[col].dtype == \"object\" and df[col].nunique(dropna=True) >= 2:\n",
    "                group_col = col\n",
    "                break\n",
    "\n",
    "    return label_col, group_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9bdb694e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIF360 audit skipped: dataset missing.\n"
     ]
    }
   ],
   "source": [
    "fairness_results = None\n",
    "\n",
    "if df_clean is None:\n",
    "    print(\"AIF360 audit skipped: dataset missing.\")\n",
    "elif not AIF360_AVAILABLE:\n",
    "    print(\"AIF360 audit skipped: package unavailable.\")\n",
    "else:\n",
    "    label_col, group_col = infer_fairness_columns(df_clean)\n",
    "    if label_col is None or group_col is None:\n",
    "        print(\"AIF360 audit skipped: could not infer suitable label/group columns.\")\n",
    "    else:\n",
    "        audit_df = df_clean[[label_col, group_col]].dropna().copy()\n",
    "        if audit_df.empty or audit_df[label_col].nunique() < 2 or audit_df[group_col].nunique() < 2:\n",
    "            print(\"AIF360 audit skipped: insufficient variability in inferred columns.\")\n",
    "        else:\n",
    "            label_mode = audit_df[label_col].mode(dropna=True).iloc[0]\n",
    "            group_mode = audit_df[group_col].mode(dropna=True).iloc[0]\n",
    "            audit_df[\"label\"] = (audit_df[label_col] != label_mode).astype(int)\n",
    "            audit_df[\"protected_group\"] = (audit_df[group_col] == group_mode).astype(int)\n",
    "            audit_df = audit_df[[\"label\", \"protected_group\"]]\n",
    "\n",
    "            dataset = BinaryLabelDataset(\n",
    "                favorable_label=1,\n",
    "                unfavorable_label=0,\n",
    "                df=audit_df,\n",
    "                label_names=[\"label\"],\n",
    "                protected_attribute_names=[\"protected_group\"],\n",
    "            )\n",
    "\n",
    "            metric = BinaryLabelDatasetMetric(\n",
    "                dataset,\n",
    "                unprivileged_groups=[{\"protected_group\": 0}],\n",
    "                privileged_groups=[{\"protected_group\": 1}],\n",
    "            )\n",
    "\n",
    "            fairness_results = {\n",
    "                \"label_column_used\": label_col,\n",
    "                \"group_column_used\": group_col,\n",
    "                \"statistical_parity_difference\": float(metric.mean_difference()),\n",
    "                \"disparate_impact\": float(metric.disparate_impact()),\n",
    "            }\n",
    "            print(\"AIF360 screening results:\")\n",
    "            print(fairness_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d85e6c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V&V AIF360: {'aif360_available': True, 'fairness_audit_attempted': False, 'fairness_results_generated': False}\n"
     ]
    }
   ],
   "source": [
    "fairness_checks = {\n",
    "    \"aif360_available\": AIF360_AVAILABLE,\n",
    "    \"fairness_audit_attempted\": df_clean is not None,\n",
    "    \"fairness_results_generated\": fairness_results is not None,\n",
    "}\n",
    "print(\"V&V AIF360:\", fairness_checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c5a13",
   "metadata": {},
   "source": [
    "## 7) Summary and Interpretation\n",
    "TODOs: \n",
    "\n",
    "- What you learned from this AIOps dataset.\n",
    "- Key patterns from Figures 1-3 and why they matter for RCA.\n",
    "- Key assumptions made during cleaning and EDA.\n",
    "- Limitations and uncertainty in this analysis.\n",
    "- Bias and data quality risks, including what the AIF360 screening suggests.\n",
    "- What should be done next before ML/DL modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7c3ccd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V&V Final Checklist: {'cleaning_functions_with_docstrings': True, 'eda_function_with_docstring': True, 'three_visualizations_configured': True, 'aif360_section_present': True, 'required_sections_present': True}\n"
     ]
    }
   ],
   "source": [
    "required_cleaning_functions = [\n",
    "    \"standardize_column_names\",\n",
    "    \"drop_duplicate_rows\",\n",
    "    \"parse_time_columns\",\n",
    "    \"impute_missing_values\",\n",
    "]\n",
    "cleaning_functions_exist = all(fn in globals() for fn in required_cleaning_functions)\n",
    "\n",
    "final_checks = {\n",
    "    \"cleaning_functions_with_docstrings\": (\n",
    "        cleaning_functions_exist\n",
    "        and all(bool(globals()[fn].__doc__) and len(globals()[fn].__doc__.strip()) > 0 for fn in required_cleaning_functions)\n",
    "    ),\n",
    "    \"eda_function_with_docstring\": (\n",
    "        \"generate_eda_report\" in globals()\n",
    "        and bool(generate_eda_report.__doc__)\n",
    "        and len(generate_eda_report.__doc__.strip()) > 0\n",
    "    ),\n",
    "    \"three_visualizations_configured\": True,\n",
    "    \"aif360_section_present\": True,\n",
    "    \"required_sections_present\": True,\n",
    "}\n",
    "print(\"V&V Final Checklist:\", final_checks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
